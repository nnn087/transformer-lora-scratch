# Transformer & LoRA Implementation from Scratch 
### [ğŸ‡¯ğŸ‡µ æ—¥æœ¬èª](#æ—¥æœ¬èª-japanese) | [ğŸ‡ºğŸ‡¸ English](#english)

-----


## ğŸ‡¯ğŸ‡µ æ—¥æœ¬èª (Japanese) <a id="æ—¥æœ¬èª-japanese"></a>

## 1\. æ¦‚è¦

æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€è«–æ–‡ã€Œ*Attention Is All You Need*ã€ãŠã‚ˆã³ã€Œ*LoRA: Low-Rank Adaptation of Large Language Models*ã€ã®ç†è§£ã‚’æ·±ã‚ã‚‹ãŸã‚ã«ã€**Hugging Faceç­‰ã®é«˜ãƒ¬ãƒ™ãƒ«ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«ä¾å­˜ã›ãšã€PyTorchã®ã¿ã§ã‚¹ã‚¯ãƒ©ãƒƒãƒå®Ÿè£…ã‚’è¡Œã£ãŸå­¦ç¿’è¨˜éŒ²**ã§ã™ã€‚

å®Ÿå‹™ãƒ¬ãƒ™ãƒ«ã®æœ€é©åŒ–ã‚ˆã‚Šã‚‚ã€ã€Œã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å†…éƒ¨æŒ™å‹•ï¼ˆæ•°å¼ã¨ã‚³ãƒ¼ãƒ‰ã®å¯¾å¿œï¼‰ã‚’è‚Œæ„Ÿè¦šã§å®Œå…¨ã«ç†è§£ã™ã‚‹ã“ã¨ã€ã‚’æœ€å„ªå…ˆã®ç›®çš„ã¨ã—ã¦è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚

> **æ³¨è¨˜:** æœ¬ãƒªãƒã‚¸ãƒˆãƒªã¯å­¦ç¿’ç›®çš„ã§ä½œæˆã•ã‚ŒãŸã‚‚ã®ã§ã‚ã‚Šã€å®Ÿå‹™ã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚ˆã‚Šã‚‚**ã‚³ãƒ¼ãƒ‰ã®å¯èª­æ€§ã¨æ•°å¼ã¨ã®å¯¾å¿œé–¢ä¿‚**ã‚’é‡è¦–ã—ã¦ã„ã¾ã™ã€‚å®Ÿç”¨çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¤ã„ã¦ã¯ã€[6.é–¢é€£ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ]ã®ãƒªãƒã‚¸ãƒˆãƒªã‚’ã”å‚ç…§ãã ã•ã„ã€‚

## 2\. æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¸»ãªç‰¹å¾´

  * **Pure PyTorch Implementation**
    `nn.Transformer` ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ç­‰ã¯ä½¿ç”¨ã›ãšã€Encoder/Decoderã‚„Attentionå±¤ã‚’ã‚¼ãƒ­ã‹ã‚‰å®šç¾©ã—ã¦ã„ã¾ã™ã€‚ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹åŒ–ã•ã‚ŒãŸå‡¦ç†ã‚’æ’é™¤ã—ã€è¨ˆç®—ã‚°ãƒ©ãƒ•ã®é€æ˜æ€§ã‚’ç¢ºä¿ã—ã¦ã„ã¾ã™ã€‚
  * **LoRA (Low-Rank Adaptation) ã®è‡ªä½œ**
    `peft` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã‚ãšã€ç·šå½¢å±¤ã¸ã®ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ— ($W + A \times B$) ã®æ³¨å…¥ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…ã—ã¾ã—ãŸã€‚å‡çµã—ãŸé‡ã¿ã¨å­¦ç¿’å¯èƒ½ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®è¨ˆç®—åˆ†é›¢ã‚’æ˜ç¢ºã«è¨˜è¿°ã—ã¦ã„ã¾ã™ã€‚
  * **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãª Attention å®Ÿè£…**
    å­¦ç¿’ç›®çš„ã®ã€Œæ‰‹å‹•è¨ˆç®—çµŒè·¯ï¼ˆæ•°å¼é€šã‚Šã®å®Ÿè£…ï¼‰ã€ã¨ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è€ƒæ…®ã—ãŸã€Œ`F.scaled_dot_product_attention` åˆ©ç”¨çµŒè·¯ã€ã®ä¸¡æ–¹ã‚’å®Ÿè£…ã—ã€æ¯”è¼ƒæ¤œè¨¼ãŒå¯èƒ½ãªè¨­è¨ˆã«ã—ã¦ã„ã¾ã™ã€‚
  * **æ§‹æˆã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–**
    å¯èª­æ€§ã¨æ‹¡å¼µæ€§ã‚’æ„è­˜ã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆã‚’æ¡ç”¨ã—ã€å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼ˆAttention, LoRA, FeedForwardï¼‰ã®å½¹å‰²ã‚’æ˜ç¢ºåŒ–ã—ã¦ã„ã¾ã™ã€‚

## 3\. ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### ã‚¹ãƒ†ãƒƒãƒ— 1: ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³ã¨ç§»å‹•

```bash
git clone https://github.com/nnn087/transformer-lora-scratch.git
cd transformer-lora-scratch
```

### ã‚¹ãƒ†ãƒƒãƒ— 2: ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

æœ¬å®Ÿè£…ã¯ PyTorch ã«ä¾å­˜ã—ã¦ã„ã¾ã™ã€‚ç’°å¢ƒã«åˆã‚ã›ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚

```bash
pip install torch numpy
```

### ã‚¹ãƒ†ãƒƒãƒ— 3: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆã®ç¢ºèª

æœ¬ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚

```text
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ attention.py  <-- Multi-Head Attention (æ‰‹å‹•è¨ˆç®—ã¨é«˜é€ŸåŒ–ã®å®Ÿè£…)
â”‚   â”‚   â”œâ”€â”€ lora.py       <-- LoRAãƒ¬ã‚¤ãƒ¤ãƒ¼ (Freezeæ¸ˆã¿é‡ã¿ + ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼)
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ transformer.py <-- ãƒ¢ãƒ‡ãƒ«å…¨ä½“çµ±åˆ
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ train.py
â””â”€â”€ README.md
```

## 4\. ä½¿ç”¨æ–¹æ³•

### ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã¨å®Ÿè¡Œ

`Transformer` ã‚¯ãƒ©ã‚¹ã‚’å‘¼ã³å‡ºã—ã€LoRAãƒ©ãƒ³ã‚¯ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§è‡ªå‹•çš„ã«ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãŒé©ç”¨ã•ã‚Œã¾ã™ã€‚

```python
import torch
from src.models.transformer import Transformer

# 1. ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ– (LoRAãƒ©ãƒ³ã‚¯æŒ‡å®šã«ã‚ˆã‚Šè‡ªå‹•ã§é©ç”¨)
model = Transformer(
    src_vocab_size=5000,
    tgt_vocab_size=5000,
    d_model=512,
    n_head=8,
    num_encoder_layers=6,
    num_decoder_layers=6,
    lora_rank=8  # LoRAã‚’æ³¨å…¥
)

# 2. ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã®å…¥åŠ› (Batch Size, Seq Len)
src = torch.randint(0, 5000, (1, 10))
tgt = torch.randint(0, 5000, (1, 10))

# 3. Forward pass
output = model(src, tgt)
print(f"Output Shape: {output.shape}") # torch.Size([1, 10, 5000])
```

## 5\. å®Ÿè£…ã®è©³ç´°ã¨é–‹ç™ºãƒ—ãƒ­ã‚»ã‚¹

### ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

ç‰¹ã«ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã€å­¦ç¿’ã¨å®Ÿè£…ã®å·¥å¤«ãŒåæ˜ ã•ã‚Œã¦ã„ã¾ã™ã€‚

  * **`src/layers/lora.py`**
    æ—¢å­˜ã® `nn.Linear` å±¤ã‚’ãƒ©ãƒƒãƒ—ã—ã€ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ— $A, B$ ã‚’æ³¨å…¥ã™ã‚‹LoRAãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å®Ÿè£…ã§ã™ã€‚
  * **`src/layers/attention.py`**
    `Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V` ã®æ•°å¼è¨ˆç®—ãƒ•ãƒ­ãƒ¼ã‚’è©³ç´°ã«ã‚³ãƒ¡ãƒ³ãƒˆã—ã€è«–æ–‡ã¨ã®å¯¾å¿œé–¢ä¿‚ã‚’è¨˜è¿°ã—ã¦ã„ã¾ã™ã€‚

### é–‹ç™ºãƒ—ãƒ­ã‚»ã‚¹ã«ã¤ã„ã¦

æœ¬å®Ÿè£…ã«ã‚ãŸã£ã¦ã¯ã€è«–æ–‡ã®èª­è§£è£œåŠ©ãŠã‚ˆã³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ãƒšã‚¢ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã¨ã—ã¦ **ç”ŸæˆAIï¼ˆLLMï¼‰** ã‚’ç©æ¥µçš„ã«æ´»ç”¨ã—ã¾ã—ãŸã€‚

ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã¯ã‚ãã¾ã§å‚è€ƒã¨ã—ã€å…¨ã¦ã®è¡Œã«ã¤ã„ã¦åŸè«–æ–‡ã®æ•°å¼ã¨ç…§åˆãƒ»ãƒ‡ãƒãƒƒã‚°ã‚’è¡Œã†ã“ã¨ã§ã€ãƒ­ã‚¸ãƒƒã‚¯ã®æ­£å½“æ€§ã‚’æ‹…ä¿ã—ã¦ã„ã¾ã™ã€‚ã€Œãªãœãã®è¨ˆç®—ã«ãªã‚‹ã®ã‹ã€ã‚’è‡ªåˆ†è‡ªèº«ã§**å’€åš¼ã—ãŸå†…å®¹**ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã¨ã—ã¦è¨˜è¿°ã—ã¦ã„ã¾ã™ã€‚

## 6\. é–¢é€£ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ

  * **[Llama 3.1 MPS Fine-tuning Repository](https://github.com/nnn087/llama31-mps-finetuning)**
    ã“ã¡ã‚‰ã¯å®Ÿç”¨ç›®çš„ã§ä½œæˆã—ãŸã€Mac (MPS) ç’°å¢ƒã§ã®Llama 3.1ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒªãƒã‚¸ãƒˆãƒªã§ã™ã€‚æœ¬ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§å¾—ãŸçŸ¥è¦‹ãŒã€ã“ã¡ã‚‰ã®æœ€é©åŒ–ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

-----

## ğŸ‡ºğŸ‡¸ English \<a id="english"\>\</a\>

## 1\. Overview

This repository contains a scratch implementation of the Transformer model based on the paper "*Attention Is All You Need*" and LoRA (Low-Rank Adaptation), implemented purely in **PyTorch**.

The project focuses on prioritizing a deep, intuitive understanding of the internal algorithms (mapping equations to code) over production-level optimization.

> **Note:** This repository is for educational and research purposes, emphasizing **code readability and equation correspondence** rather than raw performance. For practical fine-tuning, please refer to the repository in the [6.Related Projects] section.

## 2\. Key Features

  * **Pure PyTorch Implementation**
    Built the `Transformer`, `Encoder`, and `Decoder` classes from scratch without using `nn.Transformer`. This eliminates black-box processes and ensures transparency in the computation graph.
  * **Custom LoRA Implementation**
    Implemented the Low-Rank Adaptation logic ($W + A \times B$) manually without using the `peft` library. Clearly separates the computation of frozen weights and trainable adapters.
  * **Hybrid Attention Mechanism**
    Implemented both a "manual calculation path" (for learning math) and a "fast path" (using `F.scaled_dot_product_attention`) in the Attention layer to allow for comparison.
  * **Modular Design**
    Adopted a directory structure that prioritizes readability and extensibility, clarifying the role of each component (Attention, LoRA, FeedForward).

## 3\. Setup

### Step 1: Clone and Move

```bash
git clone https://github.com/nnn087/transformer-lora-scratch.git
cd transformer-lora-scratch
```

### Step 2: Install Dependencies

This implementation depends on PyTorch. Please install it according to your environment.

```bash
pip install torch numpy
```

### Step 3: Check Directory Structure

This script assumes the following directory structure:

```text
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ attention.py  <-- Multi-Head Attention (Manual & Fast imp.)
â”‚   â”‚   â”œâ”€â”€ lora.py       <-- LoRA Layer implementation
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ transformer.py <-- Integrated Model
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ train.py
â””â”€â”€ README.md
```

## 4\. Usage

### Model Initialization and Execution

Initialize the `Transformer` class and specify the LoRA rank to automatically apply adapters.

```python
import torch
from src.models.transformer import Transformer

# 1. Initialize model with LoRA
model = Transformer(
    src_vocab_size=5000,
    tgt_vocab_size=5000,
    d_model=512,
    n_head=8,
    num_encoder_layers=6,
    num_decoder_layers=6,
    lora_rank=8  # Inject LoRA
)

# 2. Dummy Input (Batch Size, Seq Len)
src = torch.randint(0, 5000, (1, 10))
tgt = torch.randint(0, 5000, (1, 10))

# 3. Forward pass
output = model(src, tgt)
print(f"Output Shape: {output.shape}") # torch.Size([1, 10, 5000])
```

## 5\. Implementation Details & Process

### Core Components

The following files reflect specific learning and implementation efforts:

  * **`src/layers/lora.py`**
    Implementation of the LoRA layer logic wrapping existing `nn.Linear` layers, injecting low-rank matrices $A$ and $B$.
    
  * **`src/layers/attention.py`**
    Multi-Head Attention implementation with detailed comments mapping code to equations (e.g., `Attention(Q, K, V)` logic).

### Development Process

I used Generative AI as a "pair programming partner" to accelerate my learning.

Instead of simply copying code, I verified every line against the original papers to ensure I understood *why* the implementation works. I have added comments documenting the content that I have personally **digested and verified**.

## 6\. Related Projects

  * **[Llama 3.1 MPS Fine-tuning Repository](https://github.com/nnn087/llama31-mps-finetuning)**
    My practical repository for fine-tuning Llama 3.1 on Mac (MPS) environment. The insights gained from this scratch implementation have been applied to optimize the practical repository.

-----
